{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Embeddings_BERT_sentence_embeddings.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ByrOX5vvSosV"},"source":["###import libraries###\n","import warnings\n","warnings.filterwarnings('ignore')\n","import pandas as pd\n","import numpy as np\n","import torch\n","#! pip install transformers\n","import transformers\n","from transformers import BertTokenizer, BertModel\n","import pickle"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YvpTS83T6qPU"},"source":["Upload data from local machine"]},{"cell_type":"code","metadata":{"colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":89},"id":"wybQdy-IScvF","executionInfo":{"status":"ok","timestamp":1606616481289,"user_tz":360,"elapsed":11009,"user":{"displayName":"Ada Jing","photoUrl":"https://lh3.googleusercontent.com/-YI2sOECfQBE/AAAAAAAAAAI/AAAAAAAAAdU/ipriLFlExBw/s64/photo.jpg","userId":"01929424035511896318"}},"outputId":"d0a2c17c-4292-490d-8beb-1ab7fd2964ab"},"source":["#upload data from local file\n","from google.colab import files\n","\n","uploaded = files.upload()\n","\n","for fn in uploaded.keys():\n","  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n","      name=fn, length=len(uploaded[fn])))\n","\n","#read in data as df\n","import io\n","df = pd.read_csv(io.BytesIO(uploaded['Cleaned_Train.csv']))"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-917cd3e6-bc5f-4edf-a6f1-b60561970fb6\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-917cd3e6-bc5f-4edf-a6f1-b60561970fb6\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving Cleaned_Train.csv to Cleaned_Train (1).csv\n","User uploaded file \"Cleaned_Train.csv\" with length 852671 bytes\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"meAaajZ06skQ"},"source":["Alternatively, we can just use the google drive"]},{"cell_type":"code","metadata":{"id":"FcepOEaW6ZzI"},"source":["train_path='/content/drive/MyDrive/MSCA Machine Learning/Data/cleaned_train.csv'\n","df=pd.read_csv(train_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Rl4ALfSUmXY"},"source":["#typecasting text input as str\n","df['text']=df['text'].astype('str')\n","#extract text input\n","text=list(df['text'])\n","# #extract test sample\n","# test=text[:10]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y8gc0yCKfwKu"},"source":["#initialkize tokenizer from base BERT\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',\n","                                          output_hidden_states = True)\n","#initialize BERT model instance\n","model = BertModel.from_pretrained('bert-base-uncased', \n","                                  return_dict=True)#return all hidden layers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F4F8y8WRTKsC","executionInfo":{"status":"ok","timestamp":1606616508103,"user_tz":360,"elapsed":395,"user":{"displayName":"Ada Jing","photoUrl":"https://lh3.googleusercontent.com/-YI2sOECfQBE/AAAAAAAAAAI/AAAAAAAAAdU/ipriLFlExBw/s64/photo.jpg","userId":"01929424035511896318"}},"outputId":"3e655055-95c9-4ff7-81e5-dbb368cbbccc"},"source":["#use to eval to ensure the model is only feeding forward\n","model.eval()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertModel(\n","  (embeddings): BertEmbeddings(\n","    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","    (position_embeddings): Embedding(512, 768)\n","    (token_type_embeddings): Embedding(2, 768)\n","    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (encoder): BertEncoder(\n","    (layer): ModuleList(\n","      (0): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (1): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (2): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (3): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (4): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (5): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (6): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (7): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (8): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (9): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (10): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (11): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (pooler): BertPooler(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"WeYXeZXVwbY6"},"source":["Below is a step-by-step demo of the embedding creation on a relatively small sample from training input (first 10 sentences)"]},{"cell_type":"code","metadata":{"id":"pZNTPmZ5eor5"},"source":["#batch tokenize test input\n","test_batch = tokenizer(\n","    test,\n","    padding=True,\n","    truncation=True,\n","    return_tensors=\"pt\"\n",")\n","#fitting tokens through model\n","with torch.no_grad():\n","\n","    outputs_batch = model(**test_batch)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JywVfDxkZyA0","executionInfo":{"status":"ok","timestamp":1606616040568,"user_tz":360,"elapsed":244,"user":{"displayName":"Ada Jing","photoUrl":"https://lh3.googleusercontent.com/-YI2sOECfQBE/AAAAAAAAAAI/AAAAAAAAAdU/ipriLFlExBw/s64/photo.jpg","userId":"01929424035511896318"}},"outputId":"26621486-31c5-4f72-b01f-5621a15411f4"},"source":["#access hidden layer from model output\n","hidden_states_batch=outputs_batch[2]\n","#access only the second to last hidden layer\n","tokens_batch=hidden_states_batch[-2]\n","#check size; size correspondes to batch_size, number_of_tokens, and feature_number\n","tokens_batch.size()\n","#creating test embedding by taking the mean of feature per token for each sentence \n","embeddings_batch = torch.mean(tokens_batch, dim=1, keepdim=True)\n","#test embedding size\n","print ('test batch embedding size is:', embeddings_batch.size())\n","#sqeeze the tensor to eliminate the \"1\" \n","embeddings_batch=torch.squeeze(embeddings_batch, dim=1)\n","#test embedding size\n","print ('test batch embedding size is:', embeddings_batch.size())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["test batch embedding size is: torch.Size([10, 1, 768])\n","test batch embedding size is: torch.Size([10, 768])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0LE-MBeazz82"},"source":["## Below will likely crash GPU runtime on Google Colab so I suggest running it on MSCA GPU instead"]},{"cell_type":"code","metadata":{"id":"GAq7zX62xzYz"},"source":["#batch tokenize test input\n","full_batch = tokenizer(\n","    text,\n","    padding=True,\n","    truncation=True,\n","    return_tensors=\"pt\"\n",")\n","#fitting tokens through model\n","with torch.no_grad():\n","    output_full = model(**full_batch)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LwlPSdE3zV4r"},"source":["#access hidden layer from model output\n","hidden_states_full=output_full[2]\n","#access only the second to last hidden layer\n","tokens_batch_full=hidden_states_full[-2]\n","#creating full embedding by taking the mean of feature per token for each sentence \n","embeddings_full = torch.mean(tokens_batch_full, dim=1, keepdim=True)\n","#full embedding size\n","print ('full batch embedding size is:', embeddings_full.size())\n","#sqeeze the tensor to eliminate the \"1\" \n","embeddings_full=torch.squeeze(embeddings_full, dim=1)\n","#full embedding size\n","print ('full batch embedding size is:', embeddings_full.size())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VKzEQDdnjpi3"},"source":["## Bert-as-a-service \n","\n","this is what we ended up using to actually produce the embedding - essentially an automated and more efficient way to carry out the above steps."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l80xl6qlXTRO","executionInfo":{"status":"ok","timestamp":1606612227730,"user_tz":360,"elapsed":15911,"user":{"displayName":"Ada Jing","photoUrl":"https://lh3.googleusercontent.com/-YI2sOECfQBE/AAAAAAAAAAI/AAAAAAAAAdU/ipriLFlExBw/s64/photo.jpg","userId":"01929424035511896318"}},"outputId":"e4e09ad4-e298-4d9a-8a9f-945611dce024"},"source":["!pip install bert-serving-client\n","!pip install -U bert-serving-server[http]\n","!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n","!unzip uncased_L-12_H-768_A-12.zip\n","%tensorflow_version 1.x\n","!nohup bert-serving-start -model_dir=./uncased_L-12_H-768_A-12 > out.file 2>&1 &"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting bert-serving-client\n","  Downloading https://files.pythonhosted.org/packages/1f/09/aae1405378a848b2e87769ad89a43d6d71978c4e15534ca48e82e723a72f/bert_serving_client-1.10.0-py2.py3-none-any.whl\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from bert-serving-client) (1.18.5)\n","Requirement already satisfied: pyzmq>=17.1.0 in /usr/local/lib/python3.6/dist-packages (from bert-serving-client) (20.0.0)\n","Installing collected packages: bert-serving-client\n","Successfully installed bert-serving-client-1.10.0\n","Collecting bert-serving-server[http]\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/bd/cab677bbd0c5fb08b72e468371d2bca6ed9507785739b4656b0b5470d90b/bert_serving_server-1.10.0-py3-none-any.whl (61kB)\n","\u001b[K     |████████████████████████████████| 71kB 7.8MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: pyzmq>=17.1.0 in /usr/local/lib/python3.6/dist-packages (from bert-serving-server[http]) (20.0.0)\n","Requirement already satisfied, skipping upgrade: termcolor>=1.1 in /usr/local/lib/python3.6/dist-packages (from bert-serving-server[http]) (1.1.0)\n","Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from bert-serving-server[http]) (1.15.0)\n","Collecting GPUtil>=1.3.0\n","  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n","Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from bert-serving-server[http]) (1.18.5)\n","Collecting flask-compress; extra == \"http\"\n","  Downloading https://files.pythonhosted.org/packages/b2/7a/9c4641f975fb9daaf945dc39da6a52fd5693ab3bbc2d53780eab3b5106f4/Flask_Compress-1.8.0-py3-none-any.whl\n","Requirement already satisfied, skipping upgrade: bert-serving-client; extra == \"http\" in /usr/local/lib/python3.6/dist-packages (from bert-serving-server[http]) (1.10.0)\n","Collecting flask-json; extra == \"http\"\n","  Downloading https://files.pythonhosted.org/packages/6f/2d/4c21d98b11f3a206fabbdd965b53a2ca3ee9fab7646c93cf36c060e8f1a4/Flask_JSON-0.3.4-py3-none-any.whl\n","Collecting flask-cors; extra == \"http\"\n","  Downloading https://files.pythonhosted.org/packages/69/7f/d0aeaaafb5c3c76c8d2141dbe2d4f6dca5d6c31872d4e5349768c1958abc/Flask_Cors-3.0.9-py2.py3-none-any.whl\n","Requirement already satisfied, skipping upgrade: flask; extra == \"http\" in /usr/local/lib/python3.6/dist-packages (from bert-serving-server[http]) (1.1.2)\n","Collecting brotli\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/d3/7c98f05b7b9103e2f3a112ba42f269c798155b3e5404fb80bb8f823aaebe/Brotli-1.0.9-cp36-cp36m-manylinux1_x86_64.whl (357kB)\n","\u001b[K     |████████████████████████████████| 358kB 29.1MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask; extra == \"http\"->bert-serving-server[http]) (7.1.2)\n","Requirement already satisfied, skipping upgrade: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from flask; extra == \"http\"->bert-serving-server[http]) (2.11.2)\n","Requirement already satisfied, skipping upgrade: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask; extra == \"http\"->bert-serving-server[http]) (1.1.0)\n","Requirement already satisfied, skipping upgrade: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from flask; extra == \"http\"->bert-serving-server[http]) (1.0.1)\n","Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->flask; extra == \"http\"->bert-serving-server[http]) (1.1.1)\n","Building wheels for collected packages: GPUtil\n","  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for GPUtil: filename=GPUtil-1.4.0-cp36-none-any.whl size=7411 sha256=21c072029fb6bb6b5caa75e9335f2eb4c6ff7509663de3b659044deb8dbdc715\n","  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n","Successfully built GPUtil\n","Installing collected packages: GPUtil, brotli, flask-compress, flask-json, flask-cors, bert-serving-server\n","Successfully installed GPUtil-1.4.0 bert-serving-server-1.10.0 brotli-1.0.9 flask-compress-1.8.0 flask-cors-3.0.9 flask-json-0.3.4\n","--2020-11-29 01:10:18--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n","Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.7.240, 172.217.13.240, 172.217.15.80, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.7.240|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 407727028 (389M) [application/zip]\n","Saving to: ‘uncased_L-12_H-768_A-12.zip’\n","\n","uncased_L-12_H-768_ 100%[===================>] 388.84M  99.7MB/s    in 3.9s    \n","\n","2020-11-29 01:10:22 (99.7 MB/s) - ‘uncased_L-12_H-768_A-12.zip’ saved [407727028/407727028]\n","\n","Archive:  uncased_L-12_H-768_A-12.zip\n","   creating: uncased_L-12_H-768_A-12/\n","  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.meta  \n","  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n","  inflating: uncased_L-12_H-768_A-12/vocab.txt  \n","  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.index  \n","  inflating: uncased_L-12_H-768_A-12/bert_config.json  \n","TensorFlow 1.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"A2iEaT9okEsM"},"source":["#create BertClient\n","from bert_serving.client import BertClient\n","bc = BertClient()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K7pcEMhYnRKk"},"source":["#create embeddings on train data\n","emb=bc.encode(text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kfg9cjtYqWEq"},"source":["#saving the embeddings in pickle\n","import pickle\n","with open('training_embeddings.pkl', 'wb') as fid:\n","     pickle.dump(emb, fid)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"audRy3f-n4Qu"},"source":["#uploading the embeddings to the 'data' google drive folder\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","# 1. Authenticate and create the PyDrive client.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)  \n","# get the folder id where you want to save your file\n","folder_id='1WBIJA0XTNaCrSxqH8Jc7I29FaNiwpH1u'\n","file = drive.CreateFile({'parents':[{u'id': folder_id}]})\n","file.SetContentFile('training_embeddings.pkl')\n","file.Upload() \n","#access the uploaded embeddings\n","from google.colab import drive\n","drive.mount(\"/content/drive\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3V3naDbK4w_H"},"source":["import pickle\n","path=\"/content/drive/MyDrive/MSCA Machine Learning/Data/training_embeddings_with_target.pkl\"\n","infile=open(path,'rb')\n","df_emb=pickle.load(infile)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":253},"id":"NHqNNCoIHiQj","executionInfo":{"status":"ok","timestamp":1606705626424,"user_tz":360,"elapsed":521,"user":{"displayName":"Ada Jing","photoUrl":"https://lh3.googleusercontent.com/-YI2sOECfQBE/AAAAAAAAAAI/AAAAAAAAAdU/ipriLFlExBw/s64/photo.jpg","userId":"01929424035511896318"}},"outputId":"b221cfaf-9947-4494-de52-6f6b7089933b"},"source":["df_emb.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>10</th>\n","      <th>11</th>\n","      <th>12</th>\n","      <th>13</th>\n","      <th>14</th>\n","      <th>15</th>\n","      <th>16</th>\n","      <th>17</th>\n","      <th>18</th>\n","      <th>19</th>\n","      <th>20</th>\n","      <th>21</th>\n","      <th>22</th>\n","      <th>23</th>\n","      <th>24</th>\n","      <th>25</th>\n","      <th>26</th>\n","      <th>27</th>\n","      <th>28</th>\n","      <th>29</th>\n","      <th>30</th>\n","      <th>31</th>\n","      <th>32</th>\n","      <th>33</th>\n","      <th>34</th>\n","      <th>35</th>\n","      <th>36</th>\n","      <th>37</th>\n","      <th>38</th>\n","      <th>39</th>\n","      <th>...</th>\n","      <th>729</th>\n","      <th>730</th>\n","      <th>731</th>\n","      <th>732</th>\n","      <th>733</th>\n","      <th>734</th>\n","      <th>735</th>\n","      <th>736</th>\n","      <th>737</th>\n","      <th>738</th>\n","      <th>739</th>\n","      <th>740</th>\n","      <th>741</th>\n","      <th>742</th>\n","      <th>743</th>\n","      <th>744</th>\n","      <th>745</th>\n","      <th>746</th>\n","      <th>747</th>\n","      <th>748</th>\n","      <th>749</th>\n","      <th>750</th>\n","      <th>751</th>\n","      <th>752</th>\n","      <th>753</th>\n","      <th>754</th>\n","      <th>755</th>\n","      <th>756</th>\n","      <th>757</th>\n","      <th>758</th>\n","      <th>759</th>\n","      <th>760</th>\n","      <th>761</th>\n","      <th>762</th>\n","      <th>763</th>\n","      <th>764</th>\n","      <th>765</th>\n","      <th>766</th>\n","      <th>767</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.174006</td>\n","      <td>0.514110</td>\n","      <td>0.282695</td>\n","      <td>-0.865944</td>\n","      <td>-0.262950</td>\n","      <td>0.222027</td>\n","      <td>0.995218</td>\n","      <td>0.638477</td>\n","      <td>-0.662021</td>\n","      <td>-0.256679</td>\n","      <td>0.340827</td>\n","      <td>-0.148740</td>\n","      <td>-0.507322</td>\n","      <td>0.185809</td>\n","      <td>-0.695070</td>\n","      <td>0.263941</td>\n","      <td>-0.073811</td>\n","      <td>0.211314</td>\n","      <td>-0.320379</td>\n","      <td>0.282107</td>\n","      <td>0.190081</td>\n","      <td>0.204111</td>\n","      <td>0.015330</td>\n","      <td>-0.084210</td>\n","      <td>0.191036</td>\n","      <td>0.191454</td>\n","      <td>-0.310451</td>\n","      <td>0.441010</td>\n","      <td>-0.497746</td>\n","      <td>-0.305810</td>\n","      <td>0.202677</td>\n","      <td>1.308998</td>\n","      <td>0.080589</td>\n","      <td>-0.039485</td>\n","      <td>-0.503274</td>\n","      <td>0.056791</td>\n","      <td>0.226311</td>\n","      <td>0.082208</td>\n","      <td>-0.402900</td>\n","      <td>1.006173</td>\n","      <td>...</td>\n","      <td>-0.235238</td>\n","      <td>-0.388879</td>\n","      <td>0.073767</td>\n","      <td>0.008587</td>\n","      <td>-0.539857</td>\n","      <td>-0.047739</td>\n","      <td>-0.010281</td>\n","      <td>-0.419728</td>\n","      <td>0.129050</td>\n","      <td>0.420352</td>\n","      <td>-0.248818</td>\n","      <td>0.352863</td>\n","      <td>0.034175</td>\n","      <td>-0.115802</td>\n","      <td>-0.190596</td>\n","      <td>-0.017529</td>\n","      <td>-0.133006</td>\n","      <td>0.238757</td>\n","      <td>-0.648669</td>\n","      <td>0.179064</td>\n","      <td>0.249245</td>\n","      <td>0.387309</td>\n","      <td>-0.379289</td>\n","      <td>0.369220</td>\n","      <td>-0.478311</td>\n","      <td>-0.315277</td>\n","      <td>0.202589</td>\n","      <td>0.094307</td>\n","      <td>0.506694</td>\n","      <td>-0.038249</td>\n","      <td>-0.348296</td>\n","      <td>0.509817</td>\n","      <td>0.030843</td>\n","      <td>0.014934</td>\n","      <td>0.092533</td>\n","      <td>0.325911</td>\n","      <td>-0.224219</td>\n","      <td>0.322711</td>\n","      <td>-0.324294</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-0.083791</td>\n","      <td>-0.074988</td>\n","      <td>-0.035012</td>\n","      <td>-0.029141</td>\n","      <td>0.765842</td>\n","      <td>-0.028787</td>\n","      <td>0.210944</td>\n","      <td>0.560736</td>\n","      <td>-0.508720</td>\n","      <td>-0.078661</td>\n","      <td>0.015503</td>\n","      <td>-0.240076</td>\n","      <td>0.030300</td>\n","      <td>0.505167</td>\n","      <td>-0.771939</td>\n","      <td>0.769955</td>\n","      <td>-0.189150</td>\n","      <td>0.045849</td>\n","      <td>0.408051</td>\n","      <td>-0.184479</td>\n","      <td>-0.426469</td>\n","      <td>-0.456381</td>\n","      <td>0.211968</td>\n","      <td>0.546523</td>\n","      <td>0.081309</td>\n","      <td>0.353223</td>\n","      <td>0.061038</td>\n","      <td>0.267759</td>\n","      <td>0.023705</td>\n","      <td>0.467865</td>\n","      <td>-0.119265</td>\n","      <td>0.135602</td>\n","      <td>0.667777</td>\n","      <td>-0.682075</td>\n","      <td>0.025304</td>\n","      <td>-0.404650</td>\n","      <td>0.276614</td>\n","      <td>0.250135</td>\n","      <td>-0.095028</td>\n","      <td>-0.023242</td>\n","      <td>...</td>\n","      <td>-0.760515</td>\n","      <td>0.066294</td>\n","      <td>0.237353</td>\n","      <td>-0.000660</td>\n","      <td>-1.157665</td>\n","      <td>-0.138523</td>\n","      <td>-0.487236</td>\n","      <td>0.171328</td>\n","      <td>0.237592</td>\n","      <td>-0.005984</td>\n","      <td>0.211551</td>\n","      <td>-0.029149</td>\n","      <td>0.314136</td>\n","      <td>0.258066</td>\n","      <td>-0.056516</td>\n","      <td>-0.055411</td>\n","      <td>0.407631</td>\n","      <td>0.270228</td>\n","      <td>-0.139126</td>\n","      <td>-0.197396</td>\n","      <td>-0.090131</td>\n","      <td>0.154644</td>\n","      <td>-0.219606</td>\n","      <td>-0.070140</td>\n","      <td>-0.303984</td>\n","      <td>0.400976</td>\n","      <td>-0.304344</td>\n","      <td>-0.343100</td>\n","      <td>-0.504064</td>\n","      <td>-0.297995</td>\n","      <td>0.050977</td>\n","      <td>-0.087849</td>\n","      <td>-0.461415</td>\n","      <td>0.217056</td>\n","      <td>-0.090924</td>\n","      <td>0.252410</td>\n","      <td>-0.205355</td>\n","      <td>-0.545921</td>\n","      <td>0.009871</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-0.026818</td>\n","      <td>0.038434</td>\n","      <td>0.296934</td>\n","      <td>-0.365698</td>\n","      <td>0.761760</td>\n","      <td>-0.738163</td>\n","      <td>0.362888</td>\n","      <td>0.261955</td>\n","      <td>-0.415269</td>\n","      <td>-0.113452</td>\n","      <td>-0.358875</td>\n","      <td>-0.566275</td>\n","      <td>-0.225447</td>\n","      <td>0.116963</td>\n","      <td>-0.295379</td>\n","      <td>0.501628</td>\n","      <td>-0.085105</td>\n","      <td>0.161736</td>\n","      <td>-0.193539</td>\n","      <td>0.161830</td>\n","      <td>0.297279</td>\n","      <td>-0.246689</td>\n","      <td>-0.638786</td>\n","      <td>-0.089641</td>\n","      <td>0.471771</td>\n","      <td>-0.113421</td>\n","      <td>0.154899</td>\n","      <td>-0.536939</td>\n","      <td>-0.527800</td>\n","      <td>-0.207359</td>\n","      <td>0.385253</td>\n","      <td>0.422704</td>\n","      <td>-0.034320</td>\n","      <td>-0.348092</td>\n","      <td>0.204914</td>\n","      <td>0.362816</td>\n","      <td>-0.023397</td>\n","      <td>0.142115</td>\n","      <td>-0.391459</td>\n","      <td>0.440117</td>\n","      <td>...</td>\n","      <td>-0.578197</td>\n","      <td>0.180630</td>\n","      <td>0.137681</td>\n","      <td>-0.439090</td>\n","      <td>-1.294470</td>\n","      <td>0.122669</td>\n","      <td>-0.545837</td>\n","      <td>0.472996</td>\n","      <td>0.241338</td>\n","      <td>0.456990</td>\n","      <td>0.111797</td>\n","      <td>0.166603</td>\n","      <td>-0.260541</td>\n","      <td>0.131190</td>\n","      <td>-0.260267</td>\n","      <td>-0.198128</td>\n","      <td>0.329539</td>\n","      <td>0.036097</td>\n","      <td>0.547372</td>\n","      <td>-0.036892</td>\n","      <td>0.041762</td>\n","      <td>0.284138</td>\n","      <td>0.193216</td>\n","      <td>0.376690</td>\n","      <td>0.173393</td>\n","      <td>-0.182035</td>\n","      <td>-0.321835</td>\n","      <td>-0.256236</td>\n","      <td>-0.342410</td>\n","      <td>-0.127833</td>\n","      <td>-0.430574</td>\n","      <td>0.259555</td>\n","      <td>-0.156196</td>\n","      <td>0.047626</td>\n","      <td>-0.047696</td>\n","      <td>-0.631763</td>\n","      <td>-0.277038</td>\n","      <td>-0.518288</td>\n","      <td>0.202233</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.070891</td>\n","      <td>-0.082333</td>\n","      <td>0.146175</td>\n","      <td>-0.132793</td>\n","      <td>0.334057</td>\n","      <td>-0.225144</td>\n","      <td>0.091203</td>\n","      <td>0.108324</td>\n","      <td>-0.622823</td>\n","      <td>0.169655</td>\n","      <td>0.052120</td>\n","      <td>-0.058048</td>\n","      <td>-0.199471</td>\n","      <td>0.474092</td>\n","      <td>-0.658516</td>\n","      <td>0.695095</td>\n","      <td>0.046220</td>\n","      <td>-0.092253</td>\n","      <td>-0.142309</td>\n","      <td>0.136454</td>\n","      <td>-0.071958</td>\n","      <td>-0.386463</td>\n","      <td>0.134039</td>\n","      <td>0.261956</td>\n","      <td>0.298472</td>\n","      <td>-0.621780</td>\n","      <td>0.282361</td>\n","      <td>-0.507688</td>\n","      <td>-0.128737</td>\n","      <td>0.230739</td>\n","      <td>0.347821</td>\n","      <td>0.342002</td>\n","      <td>-0.075256</td>\n","      <td>-0.104258</td>\n","      <td>-0.141361</td>\n","      <td>0.372969</td>\n","      <td>0.284278</td>\n","      <td>-0.276344</td>\n","      <td>-0.104445</td>\n","      <td>0.508539</td>\n","      <td>...</td>\n","      <td>-0.468448</td>\n","      <td>-0.128427</td>\n","      <td>0.294985</td>\n","      <td>-0.148065</td>\n","      <td>-1.003905</td>\n","      <td>0.224237</td>\n","      <td>-0.224838</td>\n","      <td>0.181639</td>\n","      <td>0.619375</td>\n","      <td>0.085027</td>\n","      <td>0.217503</td>\n","      <td>-0.080607</td>\n","      <td>0.312273</td>\n","      <td>-0.033001</td>\n","      <td>-0.005642</td>\n","      <td>0.226486</td>\n","      <td>0.517302</td>\n","      <td>-0.203843</td>\n","      <td>0.072023</td>\n","      <td>0.375318</td>\n","      <td>0.067540</td>\n","      <td>-0.793145</td>\n","      <td>0.242947</td>\n","      <td>0.023721</td>\n","      <td>0.040188</td>\n","      <td>-0.408251</td>\n","      <td>-0.039337</td>\n","      <td>-0.518061</td>\n","      <td>-0.200081</td>\n","      <td>-0.042007</td>\n","      <td>0.207769</td>\n","      <td>0.154214</td>\n","      <td>-0.141324</td>\n","      <td>0.320018</td>\n","      <td>-0.010292</td>\n","      <td>-0.148020</td>\n","      <td>-0.443816</td>\n","      <td>0.007930</td>\n","      <td>-0.250166</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.447092</td>\n","      <td>-0.136055</td>\n","      <td>0.358419</td>\n","      <td>0.133981</td>\n","      <td>0.344892</td>\n","      <td>-0.485013</td>\n","      <td>0.121361</td>\n","      <td>0.636551</td>\n","      <td>-0.005755</td>\n","      <td>0.001269</td>\n","      <td>0.169808</td>\n","      <td>-1.044109</td>\n","      <td>0.071822</td>\n","      <td>0.576235</td>\n","      <td>-0.638417</td>\n","      <td>0.647767</td>\n","      <td>0.020660</td>\n","      <td>0.508155</td>\n","      <td>-0.441675</td>\n","      <td>0.130068</td>\n","      <td>0.013416</td>\n","      <td>-0.327795</td>\n","      <td>-0.040595</td>\n","      <td>-0.006035</td>\n","      <td>0.503706</td>\n","      <td>-0.073351</td>\n","      <td>-0.112548</td>\n","      <td>0.344733</td>\n","      <td>-0.455994</td>\n","      <td>0.451291</td>\n","      <td>0.164639</td>\n","      <td>0.480351</td>\n","      <td>0.712888</td>\n","      <td>-0.305288</td>\n","      <td>-0.130851</td>\n","      <td>-0.360322</td>\n","      <td>0.081524</td>\n","      <td>-0.176666</td>\n","      <td>-0.193298</td>\n","      <td>0.305924</td>\n","      <td>...</td>\n","      <td>-0.333730</td>\n","      <td>-0.205804</td>\n","      <td>0.424565</td>\n","      <td>-0.236491</td>\n","      <td>-0.100342</td>\n","      <td>-0.294160</td>\n","      <td>-0.064662</td>\n","      <td>0.278276</td>\n","      <td>-0.008216</td>\n","      <td>0.030658</td>\n","      <td>0.237454</td>\n","      <td>0.141825</td>\n","      <td>-0.022997</td>\n","      <td>-0.433890</td>\n","      <td>-0.002547</td>\n","      <td>-0.533061</td>\n","      <td>0.284478</td>\n","      <td>0.135287</td>\n","      <td>0.067483</td>\n","      <td>-0.002375</td>\n","      <td>0.051481</td>\n","      <td>-0.301331</td>\n","      <td>-0.304217</td>\n","      <td>0.316481</td>\n","      <td>0.685986</td>\n","      <td>0.048112</td>\n","      <td>-0.261427</td>\n","      <td>-0.135315</td>\n","      <td>-0.554240</td>\n","      <td>-0.071735</td>\n","      <td>-0.228345</td>\n","      <td>-0.523606</td>\n","      <td>-0.023836</td>\n","      <td>0.137001</td>\n","      <td>0.076752</td>\n","      <td>-0.542120</td>\n","      <td>-0.700136</td>\n","      <td>-0.108771</td>\n","      <td>-0.287356</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 769 columns</p>\n","</div>"],"text/plain":["          0         1         2         3  ...       765       766       767  target\n","0  0.174006  0.514110  0.282695 -0.865944  ... -0.224219  0.322711 -0.324294       1\n","1 -0.083791 -0.074988 -0.035012 -0.029141  ... -0.205355 -0.545921  0.009871       1\n","2 -0.026818  0.038434  0.296934 -0.365698  ... -0.277038 -0.518288  0.202233       1\n","3  0.070891 -0.082333  0.146175 -0.132793  ... -0.443816  0.007930 -0.250166       1\n","4  0.447092 -0.136055  0.358419  0.133981  ... -0.700136 -0.108771 -0.287356       1\n","\n","[5 rows x 769 columns]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"4YugjfV3H04z"},"source":["df_emb.to_csv('training_embeddings_with_target.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"16MY-mqEID8b"},"source":["#uploading the embeddings to the 'data' google drive folder\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","# 1. Authenticate and create the PyDrive client.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)  \n","# get the folder id where you want to save your file\n","folder_id='1WBIJA0XTNaCrSxqH8Jc7I29FaNiwpH1u'\n","file = drive.CreateFile({'parents':[{u'id': folder_id}]})\n","file.SetContentFile('training_embeddings_with_target.csv')\n","file.Upload() "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":253},"id":"7-Up3EbU6FW4","executionInfo":{"status":"ok","timestamp":1606618610117,"user_tz":360,"elapsed":295,"user":{"displayName":"Ada Jing","photoUrl":"https://lh3.googleusercontent.com/-YI2sOECfQBE/AAAAAAAAAAI/AAAAAAAAAdU/ipriLFlExBw/s64/photo.jpg","userId":"01929424035511896318"}},"outputId":"9227edde-9243-48db-a95d-d7901cc637e4"},"source":["#create embedding with prediction class\n","df_emb=pd.DataFrame(emb)\n","df_emb['target']=df['target']\n","#checking out the new embedding with target\n","df_emb.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>10</th>\n","      <th>11</th>\n","      <th>12</th>\n","      <th>13</th>\n","      <th>14</th>\n","      <th>15</th>\n","      <th>16</th>\n","      <th>17</th>\n","      <th>18</th>\n","      <th>19</th>\n","      <th>20</th>\n","      <th>21</th>\n","      <th>22</th>\n","      <th>23</th>\n","      <th>24</th>\n","      <th>25</th>\n","      <th>26</th>\n","      <th>27</th>\n","      <th>28</th>\n","      <th>29</th>\n","      <th>30</th>\n","      <th>31</th>\n","      <th>32</th>\n","      <th>33</th>\n","      <th>34</th>\n","      <th>35</th>\n","      <th>36</th>\n","      <th>37</th>\n","      <th>38</th>\n","      <th>39</th>\n","      <th>...</th>\n","      <th>729</th>\n","      <th>730</th>\n","      <th>731</th>\n","      <th>732</th>\n","      <th>733</th>\n","      <th>734</th>\n","      <th>735</th>\n","      <th>736</th>\n","      <th>737</th>\n","      <th>738</th>\n","      <th>739</th>\n","      <th>740</th>\n","      <th>741</th>\n","      <th>742</th>\n","      <th>743</th>\n","      <th>744</th>\n","      <th>745</th>\n","      <th>746</th>\n","      <th>747</th>\n","      <th>748</th>\n","      <th>749</th>\n","      <th>750</th>\n","      <th>751</th>\n","      <th>752</th>\n","      <th>753</th>\n","      <th>754</th>\n","      <th>755</th>\n","      <th>756</th>\n","      <th>757</th>\n","      <th>758</th>\n","      <th>759</th>\n","      <th>760</th>\n","      <th>761</th>\n","      <th>762</th>\n","      <th>763</th>\n","      <th>764</th>\n","      <th>765</th>\n","      <th>766</th>\n","      <th>767</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.174006</td>\n","      <td>0.514110</td>\n","      <td>0.282695</td>\n","      <td>-0.865944</td>\n","      <td>-0.262950</td>\n","      <td>0.222027</td>\n","      <td>0.995218</td>\n","      <td>0.638477</td>\n","      <td>-0.662021</td>\n","      <td>-0.256679</td>\n","      <td>0.340827</td>\n","      <td>-0.148740</td>\n","      <td>-0.507322</td>\n","      <td>0.185809</td>\n","      <td>-0.695070</td>\n","      <td>0.263941</td>\n","      <td>-0.073811</td>\n","      <td>0.211314</td>\n","      <td>-0.320379</td>\n","      <td>0.282107</td>\n","      <td>0.190081</td>\n","      <td>0.204111</td>\n","      <td>0.015330</td>\n","      <td>-0.084210</td>\n","      <td>0.191036</td>\n","      <td>0.191454</td>\n","      <td>-0.310451</td>\n","      <td>0.441010</td>\n","      <td>-0.497746</td>\n","      <td>-0.305810</td>\n","      <td>0.202677</td>\n","      <td>1.308998</td>\n","      <td>0.080589</td>\n","      <td>-0.039485</td>\n","      <td>-0.503274</td>\n","      <td>0.056791</td>\n","      <td>0.226311</td>\n","      <td>0.082208</td>\n","      <td>-0.402900</td>\n","      <td>1.006173</td>\n","      <td>...</td>\n","      <td>-0.235238</td>\n","      <td>-0.388879</td>\n","      <td>0.073767</td>\n","      <td>0.008587</td>\n","      <td>-0.539857</td>\n","      <td>-0.047739</td>\n","      <td>-0.010281</td>\n","      <td>-0.419728</td>\n","      <td>0.129050</td>\n","      <td>0.420352</td>\n","      <td>-0.248818</td>\n","      <td>0.352863</td>\n","      <td>0.034175</td>\n","      <td>-0.115802</td>\n","      <td>-0.190596</td>\n","      <td>-0.017529</td>\n","      <td>-0.133006</td>\n","      <td>0.238757</td>\n","      <td>-0.648669</td>\n","      <td>0.179064</td>\n","      <td>0.249245</td>\n","      <td>0.387309</td>\n","      <td>-0.379289</td>\n","      <td>0.369220</td>\n","      <td>-0.478311</td>\n","      <td>-0.315277</td>\n","      <td>0.202589</td>\n","      <td>0.094307</td>\n","      <td>0.506694</td>\n","      <td>-0.038249</td>\n","      <td>-0.348296</td>\n","      <td>0.509817</td>\n","      <td>0.030843</td>\n","      <td>0.014934</td>\n","      <td>0.092533</td>\n","      <td>0.325911</td>\n","      <td>-0.224219</td>\n","      <td>0.322711</td>\n","      <td>-0.324294</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-0.083791</td>\n","      <td>-0.074988</td>\n","      <td>-0.035012</td>\n","      <td>-0.029141</td>\n","      <td>0.765842</td>\n","      <td>-0.028787</td>\n","      <td>0.210944</td>\n","      <td>0.560736</td>\n","      <td>-0.508720</td>\n","      <td>-0.078661</td>\n","      <td>0.015503</td>\n","      <td>-0.240076</td>\n","      <td>0.030300</td>\n","      <td>0.505167</td>\n","      <td>-0.771939</td>\n","      <td>0.769955</td>\n","      <td>-0.189150</td>\n","      <td>0.045849</td>\n","      <td>0.408051</td>\n","      <td>-0.184479</td>\n","      <td>-0.426469</td>\n","      <td>-0.456381</td>\n","      <td>0.211968</td>\n","      <td>0.546523</td>\n","      <td>0.081309</td>\n","      <td>0.353223</td>\n","      <td>0.061038</td>\n","      <td>0.267759</td>\n","      <td>0.023705</td>\n","      <td>0.467865</td>\n","      <td>-0.119265</td>\n","      <td>0.135602</td>\n","      <td>0.667777</td>\n","      <td>-0.682075</td>\n","      <td>0.025304</td>\n","      <td>-0.404650</td>\n","      <td>0.276614</td>\n","      <td>0.250135</td>\n","      <td>-0.095028</td>\n","      <td>-0.023242</td>\n","      <td>...</td>\n","      <td>-0.760515</td>\n","      <td>0.066294</td>\n","      <td>0.237353</td>\n","      <td>-0.000660</td>\n","      <td>-1.157665</td>\n","      <td>-0.138523</td>\n","      <td>-0.487236</td>\n","      <td>0.171328</td>\n","      <td>0.237592</td>\n","      <td>-0.005984</td>\n","      <td>0.211551</td>\n","      <td>-0.029149</td>\n","      <td>0.314136</td>\n","      <td>0.258066</td>\n","      <td>-0.056516</td>\n","      <td>-0.055411</td>\n","      <td>0.407631</td>\n","      <td>0.270228</td>\n","      <td>-0.139126</td>\n","      <td>-0.197396</td>\n","      <td>-0.090131</td>\n","      <td>0.154644</td>\n","      <td>-0.219606</td>\n","      <td>-0.070140</td>\n","      <td>-0.303984</td>\n","      <td>0.400976</td>\n","      <td>-0.304344</td>\n","      <td>-0.343100</td>\n","      <td>-0.504064</td>\n","      <td>-0.297995</td>\n","      <td>0.050977</td>\n","      <td>-0.087849</td>\n","      <td>-0.461415</td>\n","      <td>0.217056</td>\n","      <td>-0.090924</td>\n","      <td>0.252410</td>\n","      <td>-0.205355</td>\n","      <td>-0.545921</td>\n","      <td>0.009871</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-0.026818</td>\n","      <td>0.038434</td>\n","      <td>0.296934</td>\n","      <td>-0.365698</td>\n","      <td>0.761760</td>\n","      <td>-0.738163</td>\n","      <td>0.362888</td>\n","      <td>0.261955</td>\n","      <td>-0.415269</td>\n","      <td>-0.113452</td>\n","      <td>-0.358875</td>\n","      <td>-0.566275</td>\n","      <td>-0.225447</td>\n","      <td>0.116963</td>\n","      <td>-0.295379</td>\n","      <td>0.501628</td>\n","      <td>-0.085105</td>\n","      <td>0.161736</td>\n","      <td>-0.193539</td>\n","      <td>0.161830</td>\n","      <td>0.297279</td>\n","      <td>-0.246689</td>\n","      <td>-0.638786</td>\n","      <td>-0.089641</td>\n","      <td>0.471771</td>\n","      <td>-0.113421</td>\n","      <td>0.154899</td>\n","      <td>-0.536939</td>\n","      <td>-0.527800</td>\n","      <td>-0.207359</td>\n","      <td>0.385253</td>\n","      <td>0.422704</td>\n","      <td>-0.034320</td>\n","      <td>-0.348092</td>\n","      <td>0.204914</td>\n","      <td>0.362816</td>\n","      <td>-0.023397</td>\n","      <td>0.142115</td>\n","      <td>-0.391459</td>\n","      <td>0.440117</td>\n","      <td>...</td>\n","      <td>-0.578197</td>\n","      <td>0.180630</td>\n","      <td>0.137681</td>\n","      <td>-0.439090</td>\n","      <td>-1.294470</td>\n","      <td>0.122669</td>\n","      <td>-0.545837</td>\n","      <td>0.472996</td>\n","      <td>0.241338</td>\n","      <td>0.456990</td>\n","      <td>0.111797</td>\n","      <td>0.166603</td>\n","      <td>-0.260541</td>\n","      <td>0.131190</td>\n","      <td>-0.260267</td>\n","      <td>-0.198128</td>\n","      <td>0.329539</td>\n","      <td>0.036097</td>\n","      <td>0.547372</td>\n","      <td>-0.036892</td>\n","      <td>0.041762</td>\n","      <td>0.284138</td>\n","      <td>0.193216</td>\n","      <td>0.376690</td>\n","      <td>0.173393</td>\n","      <td>-0.182035</td>\n","      <td>-0.321835</td>\n","      <td>-0.256236</td>\n","      <td>-0.342410</td>\n","      <td>-0.127833</td>\n","      <td>-0.430574</td>\n","      <td>0.259555</td>\n","      <td>-0.156196</td>\n","      <td>0.047626</td>\n","      <td>-0.047696</td>\n","      <td>-0.631763</td>\n","      <td>-0.277038</td>\n","      <td>-0.518288</td>\n","      <td>0.202233</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.070891</td>\n","      <td>-0.082333</td>\n","      <td>0.146175</td>\n","      <td>-0.132793</td>\n","      <td>0.334057</td>\n","      <td>-0.225144</td>\n","      <td>0.091203</td>\n","      <td>0.108324</td>\n","      <td>-0.622823</td>\n","      <td>0.169655</td>\n","      <td>0.052120</td>\n","      <td>-0.058048</td>\n","      <td>-0.199471</td>\n","      <td>0.474092</td>\n","      <td>-0.658516</td>\n","      <td>0.695095</td>\n","      <td>0.046220</td>\n","      <td>-0.092253</td>\n","      <td>-0.142309</td>\n","      <td>0.136454</td>\n","      <td>-0.071958</td>\n","      <td>-0.386463</td>\n","      <td>0.134039</td>\n","      <td>0.261956</td>\n","      <td>0.298472</td>\n","      <td>-0.621780</td>\n","      <td>0.282361</td>\n","      <td>-0.507688</td>\n","      <td>-0.128737</td>\n","      <td>0.230739</td>\n","      <td>0.347821</td>\n","      <td>0.342002</td>\n","      <td>-0.075256</td>\n","      <td>-0.104258</td>\n","      <td>-0.141361</td>\n","      <td>0.372969</td>\n","      <td>0.284278</td>\n","      <td>-0.276344</td>\n","      <td>-0.104445</td>\n","      <td>0.508539</td>\n","      <td>...</td>\n","      <td>-0.468448</td>\n","      <td>-0.128427</td>\n","      <td>0.294985</td>\n","      <td>-0.148065</td>\n","      <td>-1.003905</td>\n","      <td>0.224237</td>\n","      <td>-0.224838</td>\n","      <td>0.181639</td>\n","      <td>0.619375</td>\n","      <td>0.085027</td>\n","      <td>0.217503</td>\n","      <td>-0.080607</td>\n","      <td>0.312273</td>\n","      <td>-0.033001</td>\n","      <td>-0.005642</td>\n","      <td>0.226486</td>\n","      <td>0.517302</td>\n","      <td>-0.203843</td>\n","      <td>0.072023</td>\n","      <td>0.375318</td>\n","      <td>0.067540</td>\n","      <td>-0.793145</td>\n","      <td>0.242947</td>\n","      <td>0.023721</td>\n","      <td>0.040188</td>\n","      <td>-0.408251</td>\n","      <td>-0.039337</td>\n","      <td>-0.518061</td>\n","      <td>-0.200081</td>\n","      <td>-0.042007</td>\n","      <td>0.207769</td>\n","      <td>0.154214</td>\n","      <td>-0.141324</td>\n","      <td>0.320018</td>\n","      <td>-0.010292</td>\n","      <td>-0.148020</td>\n","      <td>-0.443816</td>\n","      <td>0.007930</td>\n","      <td>-0.250166</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.447092</td>\n","      <td>-0.136055</td>\n","      <td>0.358419</td>\n","      <td>0.133981</td>\n","      <td>0.344892</td>\n","      <td>-0.485013</td>\n","      <td>0.121361</td>\n","      <td>0.636551</td>\n","      <td>-0.005755</td>\n","      <td>0.001269</td>\n","      <td>0.169808</td>\n","      <td>-1.044109</td>\n","      <td>0.071822</td>\n","      <td>0.576235</td>\n","      <td>-0.638417</td>\n","      <td>0.647767</td>\n","      <td>0.020660</td>\n","      <td>0.508155</td>\n","      <td>-0.441675</td>\n","      <td>0.130068</td>\n","      <td>0.013416</td>\n","      <td>-0.327795</td>\n","      <td>-0.040595</td>\n","      <td>-0.006035</td>\n","      <td>0.503706</td>\n","      <td>-0.073351</td>\n","      <td>-0.112548</td>\n","      <td>0.344733</td>\n","      <td>-0.455994</td>\n","      <td>0.451291</td>\n","      <td>0.164639</td>\n","      <td>0.480351</td>\n","      <td>0.712888</td>\n","      <td>-0.305288</td>\n","      <td>-0.130851</td>\n","      <td>-0.360322</td>\n","      <td>0.081524</td>\n","      <td>-0.176666</td>\n","      <td>-0.193298</td>\n","      <td>0.305924</td>\n","      <td>...</td>\n","      <td>-0.333730</td>\n","      <td>-0.205804</td>\n","      <td>0.424565</td>\n","      <td>-0.236491</td>\n","      <td>-0.100342</td>\n","      <td>-0.294160</td>\n","      <td>-0.064662</td>\n","      <td>0.278276</td>\n","      <td>-0.008216</td>\n","      <td>0.030658</td>\n","      <td>0.237454</td>\n","      <td>0.141825</td>\n","      <td>-0.022997</td>\n","      <td>-0.433890</td>\n","      <td>-0.002547</td>\n","      <td>-0.533061</td>\n","      <td>0.284478</td>\n","      <td>0.135287</td>\n","      <td>0.067483</td>\n","      <td>-0.002375</td>\n","      <td>0.051481</td>\n","      <td>-0.301331</td>\n","      <td>-0.304217</td>\n","      <td>0.316481</td>\n","      <td>0.685986</td>\n","      <td>0.048112</td>\n","      <td>-0.261427</td>\n","      <td>-0.135315</td>\n","      <td>-0.554240</td>\n","      <td>-0.071735</td>\n","      <td>-0.228345</td>\n","      <td>-0.523606</td>\n","      <td>-0.023836</td>\n","      <td>0.137001</td>\n","      <td>0.076752</td>\n","      <td>-0.542120</td>\n","      <td>-0.700136</td>\n","      <td>-0.108771</td>\n","      <td>-0.287356</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 769 columns</p>\n","</div>"],"text/plain":["          0         1         2         3  ...       765       766       767  target\n","0  0.174006  0.514110  0.282695 -0.865944  ... -0.224219  0.322711 -0.324294       1\n","1 -0.083791 -0.074988 -0.035012 -0.029141  ... -0.205355 -0.545921  0.009871       1\n","2 -0.026818  0.038434  0.296934 -0.365698  ... -0.277038 -0.518288  0.202233       1\n","3  0.070891 -0.082333  0.146175 -0.132793  ... -0.443816  0.007930 -0.250166       1\n","4  0.447092 -0.136055  0.358419  0.133981  ... -0.700136 -0.108771 -0.287356       1\n","\n","[5 rows x 769 columns]"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"RgQ8zr7G7xdc"},"source":["#pickle full embeddings with feature\n","with open('training_embeddings_with_target.pkl', 'wb') as fid:\n","     pickle.dump(df_emb, fid)\n","\n","#uploading the embeddings to the 'data' google drive folder\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)  \n","#uploading to data folder\n","folder_id='1WBIJA0XTNaCrSxqH8Jc7I29FaNiwpH1u'\n","file = drive.CreateFile({'parents':[{u'id': folder_id}]})\n","file.SetContentFile('training_embeddings_with_target.pkl')\n","file.Upload() "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ehTX9cOtHIoA"},"source":["#access the uploaded embeddings\n","from google.colab import drive\n","drive.mount(\"/content/drive\")\n","path=\"/content/drive/MyDrive/MSCA Machine Learning/Data/training_embeddings.pkl\"\n","infile=open(path,'rb')\n","emb=pickle.load(infile)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JDhrTzw98tT-"},"source":["Below is naive bayes using TF-IDF embedding for reference only"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ArePnFU_87h-","executionInfo":{"status":"ok","timestamp":1606619610690,"user_tz":360,"elapsed":263,"user":{"displayName":"Ada Jing","photoUrl":"https://lh3.googleusercontent.com/-YI2sOECfQBE/AAAAAAAAAAI/AAAAAAAAAdU/ipriLFlExBw/s64/photo.jpg","userId":"01929424035511896318"}},"outputId":"d5c3e7c7-dc0c-4c2a-fb7a-6c7deef14358"},"source":["X=df['text']\n","y=df['target']\n","print ('intput variable shape is', X.shape)\n","print ('intput target shape is', y.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["intput variable shape is (7613,)\n","intput target shape is (7613,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2OOX7PRA8DOb"},"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mVWhN9B8-zi8"},"source":["from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfTransformer, TfidfVectorizer\n","vect = TfidfVectorizer(stop_words='english')\n","X_train_m = vect.fit_transform(X_train)\n","X_test_m = vect.transform(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nAw6dee--GlT"},"source":["from sklearn.naive_bayes import MultinomialNB\n","nb = MultinomialNB()\n","nb.fit(X_train_m, y_train)\n","y_pred=nb.predict(X_test_m)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oBUpNryY-gNa","executionInfo":{"status":"ok","timestamp":1606619707234,"user_tz":360,"elapsed":423,"user":{"displayName":"Ada Jing","photoUrl":"https://lh3.googleusercontent.com/-YI2sOECfQBE/AAAAAAAAAAI/AAAAAAAAAdU/ipriLFlExBw/s64/photo.jpg","userId":"01929424035511896318"}},"outputId":"14bf1bb8-b591-4f09-afc1-7a2aab663a6d"},"source":["from sklearn.metrics import classification_report, accuracy_score\n","# calculate accuracy of class predictions\n","print(f\"Test Accuracy: {accuracy_score(y_test, y_pred) * 100:.1f}%\")\n","print(classification_report(y_test, y_pred))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test Accuracy: 79.4%\n","              precision    recall  f1-score   support\n","\n","           0       0.76      0.91      0.83      1049\n","           1       0.86      0.65      0.74       855\n","\n","    accuracy                           0.79      1904\n","   macro avg       0.81      0.78      0.78      1904\n","weighted avg       0.80      0.79      0.79      1904\n","\n"],"name":"stdout"}]}]}