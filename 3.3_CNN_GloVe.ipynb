{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 31009 - Final Project - CNN Model\n",
    "### Ada, Rohit, Dylan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd \n",
    "import re   \n",
    "import nltk  \n",
    "from nltk.corpus import stopwords           \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from collections import Counter  \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt     \n",
    "from IPython.core.display import display, HTML  \n",
    "import string\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences   \n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU, Convolution1D, Flatten, LeakyReLU\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, GlobalAveragePooling1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tqdm import tqdm  \n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import load_model\n",
    "\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "from tensorflow.keras.utils import model_to_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Load Data\n",
    "train = pd.read_csv(\"Cleaned_Train.csv\")\n",
    "train_y = train.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 5)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 17440\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer sequence and index words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train.text)   \n",
    "word_index = tokenizer.word_index    \n",
    "num_words = len(tokenizer.word_index)+1\n",
    "print('Number of unique words:',len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sequences = tokenizer.texts_to_sequences(train.text)  \n",
    "\n",
    "\n",
    "# Ading padding at the front of text sequence\n",
    "training_padded = pad_sequences(training_sequences,                                  \n",
    "                                   maxlen=50,                                      \n",
    "                                   padding='pre',                           \n",
    "                                   truncating='pre')  \n",
    "\n",
    "# Split data set for further training and validation\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(training_padded, train_y, test_size=.25,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17440/17440 [00:00<00:00, 242764.75it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(17441, 300)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matching with Glove embedding 6B.300D\n",
    "\n",
    "embedding_dict={}\n",
    "with open('glove.6B.300d.txt','r',encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values=line.split()\n",
    "        word=values[0]\n",
    "        vectors=np.asarray(values[1:],'float32')\n",
    "        embedding_dict[word]=vectors\n",
    "f.close()\n",
    "\n",
    "embedding_dim=300\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    if i < num_words:\n",
    "        embedding_vector = embedding_dict.get(word)  \n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN1D with Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define Model Hyperparameters\n",
    "embedding_dim = 300\n",
    "\n",
    "# initial filters and filter size\n",
    "num_filters = 10\n",
    "filter_size = 3\n",
    "\n",
    "# Here we specify the number of units of our hidden layer\n",
    "hidden_dims = 5\n",
    "\n",
    "# Training batch size and epochs\n",
    "batch_size = 10\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "sequence_length = 50\n",
    "\n",
    "#Specify the number of classes to predict (1 for binary classification or count unique values for multilabel classification)\n",
    "num_classes = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 300)           5232300   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 48, 10)            9010      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 55        \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 6         \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 5,241,371\n",
      "Trainable params: 9,071\n",
      "Non-trainable params: 5,232,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Building the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words,\n",
    "            embedding_dim,\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=sequence_length,\n",
    "            trainable=False))\n",
    "\n",
    "model.add(Convolution1D(filters=10,\n",
    "                         kernel_size=3,\n",
    "                         padding=\"valid\",\n",
    "                         activation=\"sigmoid\",\n",
    "                         strides=1))\n",
    "\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(5))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.6460 - accuracy: 0.6221 - val_loss: 0.6179 - val_accuracy: 0.6786\n",
      "Epoch 2/10\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.5885 - accuracy: 0.7461 - val_loss: 0.5629 - val_accuracy: 0.7745\n",
      "Epoch 3/10\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.5288 - accuracy: 0.7937 - val_loss: 0.5163 - val_accuracy: 0.7920\n",
      "Epoch 4/10\n",
      "429/429 [==============================] - 1s 2ms/step - loss: 0.4765 - accuracy: 0.8215 - val_loss: 0.4791 - val_accuracy: 0.8004\n",
      "Epoch 5/10\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.4366 - accuracy: 0.8358 - val_loss: 0.4597 - val_accuracy: 0.7976\n",
      "Epoch 6/10\n",
      "429/429 [==============================] - 1s 2ms/step - loss: 0.4029 - accuracy: 0.8542 - val_loss: 0.4499 - val_accuracy: 0.8025\n",
      "Epoch 7/10\n",
      "429/429 [==============================] - 1s 2ms/step - loss: 0.3730 - accuracy: 0.8706 - val_loss: 0.4448 - val_accuracy: 0.8060\n",
      "Epoch 8/10\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.3469 - accuracy: 0.8785 - val_loss: 0.4463 - val_accuracy: 0.8053\n",
      "Epoch 9/10\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.3242 - accuracy: 0.8911 - val_loss: 0.4544 - val_accuracy: 0.8074\n",
      "Epoch 10/10\n",
      "429/429 [==============================] - 1s 2ms/step - loss: 0.3032 - accuracy: 0.9012 - val_loss: 0.4545 - val_accuracy: 0.7948\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 0.4589 - accuracy: 0.7941\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.45893147587776184, 0.7941176295280457]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit the mode and evaluate the model\n",
    "model_1_fit = model.fit(X_train, Y_train, validation_split=.25, epochs=10, batch_size=10)\n",
    "model.evaluate(X_test, Y_test, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "#Save model file to disk\n",
    "model_json = model.to_json()\n",
    "with open(\"cnnmodel.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"cnnmodel.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
