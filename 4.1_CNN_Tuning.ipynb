{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 31009 - Final Project - CNN Tunning OOD\n",
    "### Ada, Rohit, Dylan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd \n",
    "import re   \n",
    "import nltk  \n",
    "from nltk.corpus import stopwords           \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from collections import Counter  \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt     \n",
    "from IPython.core.display import display, HTML  \n",
    "import string\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences   \n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU, Convolution1D, Flatten, LeakyReLU\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, GlobalAveragePooling1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tqdm import tqdm  \n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import load_model\n",
    "\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "from tensorflow.keras.utils import model_to_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Load Data\n",
    "train = pd.read_csv(\"Cleaned_Train.csv\")\n",
    "train_y = train.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 17440\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer sequence and index words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train.text)   \n",
    "word_index = tokenizer.word_index    \n",
    "num_words = len(tokenizer.word_index)+1\n",
    "print('Number of unique words:',len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sequences = tokenizer.texts_to_sequences(train.text)  \n",
    "\n",
    "# Ading padding at the front of text sequence\n",
    "training_padded = pad_sequences(training_sequences,                                  \n",
    "                                   maxlen=50,                                      \n",
    "                                   padding='pre',                           \n",
    "                                   truncating='pre')  \n",
    "\n",
    "# Split data set for further training and validation\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(training_padded, train_y, test_size=.25,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17440/17440 [00:00<00:00, 499717.60it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(17441, 300)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matching with Glove embedding 6B.300D\n",
    "mbedding_dict={}\n",
    "with open('glove.6B.300d.txt','r',encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values=line.split()\n",
    "        word=values[0]\n",
    "        vectors=np.asarray(values[1:],'float32')\n",
    "        embedding_dict[word]=vectors\n",
    "f.close()\n",
    "\n",
    "embedding_dim=300\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    if i < num_words:\n",
    "        embedding_vector = embedding_dict.get(word)  \n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Tuning Process\n",
    "\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from kerastuner import HyperModel, RandomSearch, Hyperband, BayesianOptimization\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "# Model Hyperparameters\n",
    "embedding_dim = 300\n",
    "\n",
    "sequence_length = 50\n",
    "\n",
    "NUM_CLASSES = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building CNN HyperModel Class with Search Space defined\n",
    "class CNNHyperModel(HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(num_words,\n",
    "                embedding_dim,\n",
    "                weights=[embedding_matrix],\n",
    "                input_length=sequence_length,\n",
    "                trainable=False))\n",
    "        \n",
    "        #Convolution1D layer with Search Space Filter: {5,10}\n",
    "        #Convolution1D layer with Search Space Kernel Size: {2, 3 ,5}\n",
    "        \n",
    "        model.add(Convolution1D(filters=hp.Choice(\"num_filters\", values=[5, 10], default= 5),\n",
    "                             kernel_size=hp.Choice(\"kernel_size\", values=[2, 3 ,5], default= 3),\n",
    "                             padding=\"valid\",\n",
    "                             activation=\"relu\",\n",
    "                             strides=1))\n",
    "\n",
    "        model.add(GlobalMaxPooling1D())\n",
    "        \n",
    "        #Dense layer with Search Space Units: {2 ~ 10}\n",
    "        #Dense layer with Search Space activation Function: {\"relu\", \"tanh\", \"sigmoid\"}\n",
    "        model.add(\n",
    "            Dense(\n",
    "                units=hp.Int(\n",
    "                    \"units\", min_value=2, max_value=10, step=2, default = 2\n",
    "                ),\n",
    "                activation=hp.Choice(\n",
    "                    \"dense_activation\",\n",
    "                    values=[\"relu\", \"tanh\", \"sigmoid\"],\n",
    "                    default=\"relu\",\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        #Dense layer with Search Space Dropout Rate: {0 ~ 0.4}\n",
    "        model.add(\n",
    "            Dropout(\n",
    "                rate=hp.Float(\n",
    "                    \"dropout_1\", min_value=0.0, max_value=0.4, default=0.2, step=0.05,\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        model.add(Dense(1))\n",
    "        \n",
    "        #Dense layer with Search Space activation Function: {\"relu\", \"tanh\", \"sigmoid\"}\n",
    "        model.add(            \n",
    "            Dense(\n",
    "                units = 1,\n",
    "                activation = hp.Choice(\n",
    "                    \"dense_activation\",\n",
    "                    values=[\"relu\", \"tanh\", \"sigmoid\"],\n",
    "                    default=\"relu\",\n",
    "                )\n",
    "            ))\n",
    "\n",
    "        #Dense layer with Search Space for learning rate\n",
    "        model.compile(            \n",
    "            optimizer=keras.optimizers.Adam(\n",
    "                hp.Float(\n",
    "                    \"learning_rate\",\n",
    "                    min_value=1e-4,\n",
    "                    max_value=1e-2,\n",
    "                    sampling=\"LOG\",\n",
    "                    default=1e-3,\n",
    "                )\n",
    "            ),\n",
    "            loss=\"binary_crossentropy\",\n",
    "            metrics=[\"accuracy\"])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from kerastuner.tuners import (\n",
    "    BayesianOptimization,\n",
    "    Hyperband,\n",
    "    RandomSearch,\n",
    ")\n",
    "from loguru import logger\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Initial Hyperparameter tuning parameter\n",
    "SEED = 0\n",
    "\n",
    "N_EPOCH_SEARCH = 40\n",
    "HYPERBAND_MAX_EPOCHS = 40\n",
    "MAX_TRIALS = 100\n",
    "EXECUTION_PER_TRIAL = 2\n",
    "BAYESIAN_NUM_INITIAL_POINTS = 1\n",
    "\n",
    "\n",
    "# Helper Functions\n",
    "def run_hyperparameter_tuning():\n",
    "#     build the CNN model with defined search space\n",
    "    hypermodel = CNNHyperModel()\n",
    "\n",
    "#     save all history model log file\n",
    "    output_dir = Path(\"./tuning/\")\n",
    "    tuners = define_tuners(\n",
    "        hypermodel, directory=output_dir, project_name=\"simple_cnn_tuning\"\n",
    "    )\n",
    "\n",
    "#     gnerate results for each tuning method\n",
    "    results = []\n",
    "    for tuner in tuners:\n",
    "#         obtain time loss and accuracy from evaluation \n",
    "        elapsed_time, loss, accuracy = tuner_evaluation(\n",
    "            tuner, X_train, X_test, Y_train, Y_test\n",
    "        )\n",
    "        logger.info(\n",
    "            f\"Elapsed time = {elapsed_time:10.4f} s, accuracy = {accuracy}, loss = {loss}\"\n",
    "        )\n",
    "        results.append([elapsed_time, loss, accuracy])\n",
    "#     log out current tuning results\n",
    "    logger.info(results)\n",
    "\n",
    "\n",
    "def tuner_evaluation(tuner, X_train, X_test, Y_train, Y_test):\n",
    "\n",
    "    # Overview of the tuning task\n",
    "    tuner.search_space_summary()\n",
    "\n",
    "    # Performs the hyperparameter tuning\n",
    "    logger.info(\"Start hyperparameter tuning\")\n",
    "    search_start = time.time()\n",
    "    tuner.search(X_train, Y_train, epochs=N_EPOCH_SEARCH, validation_split=0.1)\n",
    "    search_end = time.time()\n",
    "    elapsed_time = search_end - search_start\n",
    "\n",
    "    # Show a summary of the search\n",
    "    tuner.results_summary()\n",
    "\n",
    "    # Retrieve the best model.\n",
    "    best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "    # Evaluate the best model\n",
    "    loss, accuracy = best_model.evaluate(X_test, Y_test)\n",
    "    return elapsed_time, loss, accuracy\n",
    "\n",
    "\n",
    "def define_tuners(hypermodel, directory, project_name):\n",
    "    \n",
    "    # Final result will be replace by bayesian tunner if run everthing together. \n",
    "    # Try comment out other two part and run though each method one by one.\n",
    "    # Please check log file for final outputs\n",
    "    \n",
    "    # Random search tuner\n",
    "    random_tuner = RandomSearch(\n",
    "        hypermodel,\n",
    "        objective=\"val_accuracy\",\n",
    "        seed=SEED,\n",
    "        max_trials=MAX_TRIALS,\n",
    "        executions_per_trial=EXECUTION_PER_TRIAL,\n",
    "        directory=f\"{directory}_random_search\",\n",
    "        project_name=project_name,\n",
    "    )\n",
    "    # Hyperband tuner\n",
    "    hyperband_tuner = Hyperband(\n",
    "        hypermodel,\n",
    "        max_epochs=HYPERBAND_MAX_EPOCHS,\n",
    "        objective=\"val_accuracy\",\n",
    "        seed=SEED,\n",
    "        executions_per_trial=EXECUTION_PER_TRIAL,\n",
    "        directory=f\"{directory}_hyperband\",\n",
    "        project_name=project_name,\n",
    "    )\n",
    "    # Bayesian tuner\n",
    "    bayesian_tuner = BayesianOptimization(\n",
    "        hypermodel,\n",
    "        objective='val_accuracy',\n",
    "        seed=SEED,\n",
    "        num_initial_points=BAYESIAN_NUM_INITIAL_POINTS,\n",
    "        max_trials=MAX_TRIALS,\n",
    "        directory=f\"{directory}_bayesian\",\n",
    "        project_name=project_name\n",
    "    )\n",
    "    return [random_tuner, hyperband_tuner, bayesian_tuner]\n",
    "#     return [random_tuner]\n",
    "#     return [hyperband_tuner]\n",
    "#     return [bayesian_tuner]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 90 Complete [00h 00m 56s]\n",
      "val_accuracy: 0.7950963079929352\n",
      "\n",
      "Best val_accuracy So Far: 0.8196147084236145\n",
      "Total elapsed time: 00h 24m 44s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Results summary\n",
      "Results in cifar10_hyperband\\simple_cnn_tuning\n",
      "Showing 10 best trials\n",
      "Objective(name='val_accuracy', direction='max')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_filters: 10\n",
      "kernel_size: 3\n",
      "units: 4\n",
      "dense_activation: sigmoid\n",
      "dropout_1: 0.25\n",
      "learning_rate: 0.0008436373753133344\n",
      "tuner/epochs: 14\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 1\n",
      "tuner/round: 0\n",
      "Score: 0.8196147084236145\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_filters: 5\n",
      "kernel_size: 5\n",
      "units: 4\n",
      "dense_activation: sigmoid\n",
      "dropout_1: 0.30000000000000004\n",
      "learning_rate: 0.00040536211023129484\n",
      "tuner/epochs: 40\n",
      "tuner/initial_epoch: 14\n",
      "tuner/bracket: 1\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 23b96fa765ae590466c2b4eab6362856\n",
      "Score: 0.8196147084236145\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_filters: 5\n",
      "kernel_size: 5\n",
      "units: 8\n",
      "dense_activation: sigmoid\n",
      "dropout_1: 0.35000000000000003\n",
      "learning_rate: 0.00015893004455259552\n",
      "tuner/epochs: 40\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 0\n",
      "tuner/round: 0\n",
      "Score: 0.8196147084236145\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_filters: 10\n",
      "kernel_size: 5\n",
      "units: 8\n",
      "dense_activation: sigmoid\n",
      "dropout_1: 0.1\n",
      "learning_rate: 0.001784589319901582\n",
      "tuner/epochs: 5\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 2\n",
      "tuner/round: 0\n",
      "Score: 0.817863404750824\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_filters: 5\n",
      "kernel_size: 5\n",
      "units: 8\n",
      "dense_activation: sigmoid\n",
      "dropout_1: 0.15000000000000002\n",
      "learning_rate: 0.002707956564249412\n",
      "tuner/epochs: 5\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 2\n",
      "tuner/round: 0\n",
      "Score: 0.8169877529144287\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_filters: 10\n",
      "kernel_size: 5\n",
      "units: 8\n",
      "dense_activation: sigmoid\n",
      "dropout_1: 0.1\n",
      "learning_rate: 0.001784589319901582\n",
      "tuner/epochs: 14\n",
      "tuner/initial_epoch: 5\n",
      "tuner/bracket: 2\n",
      "tuner/round: 1\n",
      "tuner/trial_id: c2f0689e0b05b89ec372a17bb1ac676f\n",
      "Score: 0.8161121010780334\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_filters: 5\n",
      "kernel_size: 5\n",
      "units: 8\n",
      "dense_activation: sigmoid\n",
      "dropout_1: 0.15000000000000002\n",
      "learning_rate: 0.002707956564249412\n",
      "tuner/epochs: 14\n",
      "tuner/initial_epoch: 5\n",
      "tuner/bracket: 2\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 46ebff0d75de072e9271e84206abade0\n",
      "Score: 0.8161120712757111\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_filters: 10\n",
      "kernel_size: 3\n",
      "units: 2\n",
      "dense_activation: sigmoid\n",
      "dropout_1: 0.1\n",
      "learning_rate: 0.0007068224084841514\n",
      "tuner/epochs: 40\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 0\n",
      "tuner/round: 0\n",
      "Score: 0.8152364194393158\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_filters: 5\n",
      "kernel_size: 3\n",
      "units: 4\n",
      "dense_activation: sigmoid\n",
      "dropout_1: 0.25\n",
      "learning_rate: 0.002596852274141183\n",
      "tuner/epochs: 14\n",
      "tuner/initial_epoch: 5\n",
      "tuner/bracket: 3\n",
      "tuner/round: 2\n",
      "tuner/trial_id: f4b8de482c933b28408e2a0c52c3ffef\n",
      "Score: 0.8143607676029205\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_filters: 5\n",
      "kernel_size: 5\n",
      "units: 8\n",
      "dense_activation: sigmoid\n",
      "dropout_1: 0.05\n",
      "learning_rate: 0.0025119714799088843\n",
      "tuner/epochs: 40\n",
      "tuner/initial_epoch: 14\n",
      "tuner/bracket: 1\n",
      "tuner/round: 1\n",
      "tuner/trial_id: d7df791eb345fd27b87432a359f33094\n",
      "Score: 0.8134851157665253\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.4607 - accuracy: 0.8009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-05 21:53:35.959 | INFO     | __main__:run_hyperparameter_tuning:36 - Elapsed time =  1484.1293 s, accuracy = 0.8009454011917114, loss = 0.46071144938468933\n",
      "2020-12-05 21:53:35.960 | INFO     | __main__:run_hyperparameter_tuning:39 - [[1484.129281282425, 0.46071144938468933, 0.8009454011917114]]\n"
     ]
    }
   ],
   "source": [
    "run_hyperparameter_tuning()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
